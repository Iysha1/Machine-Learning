# -*- coding: utf-8 -*-
"""FakeTrueNews

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dUjLRLh3OzXEN1Z1c9zNgNkrFXuNfshf
"""

pip install numpy

pip install pandas

pip install tensorflow

pip install seaborn

pip install matplotlib

pip install roc

pip install sklearn

pip install pandas

pip install sklearn

import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns
from wordcloud import WordCloud, STOPWORDS
import nltk
import re
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize

import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from tensorflow.keras.preprocessing.text import one_hot, Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional
from tensorflow.keras.models import Model
import warnings
warnings.filterwarnings('ignore')


import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

fakedf = pd.read_csv('/content/drive/MyDrive/Ayse/references/Fake.csv')
truedf = pd.read_csv('/content/drive/MyDrive/Ayse/references/True.csv')

fakedf.head()

truedf.head()

fakedf.isnull().sum()

truedf.isnull().sum()

fakedf.info()

truedf.info()

truedf['real'] = 1
fakedf['real'] = 0

truedf.head()

fakedf.head()

combinedf = pd.concat([truedf, fakedf]).reset_index(drop=True)

combinedf.tail(2) #son verileri gosterir

combinedf.drop(columns=['date'], axis=1,inplace=True)

combinedf.head()

combinedf['info'] = combinedf['title'] +' '+ combinedf['text']

combinedf.head()

!python -m nltk.downloader stopwords

import nltk
nltk.download('stopwords')

stopwords= stopwords.words('english')
stopwords.extend(['from','subject', 're', 'edu', 'use'])
def preprocessing(text):
  result=[]
  for token in gensim.utils.simple_preprocess(text):
    if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2 and token not in stopwords:
            result.append(token)

  return result

combinedf['cleanTitle']=combinedf['title'].apply(preprocessing)

combinedf['cleanTitle'][0]

combinedf['cleanText']=combinedf['text'].apply(preprocessing)

combinedf['cleanText'][0]

combinedf['cleanText_joined'] = combinedf['cleanText'].apply(lambda x: " " .join(x))

combinedf['cleanTitle_joined'] = combinedf['cleanTitle'].apply(lambda x: " " .join(x))

combinedf['cleanFinal'] = combinedf['info'].apply(preprocessing)
combinedf['cleanFinal_joined'] = combinedf['cleanFinal'].apply(lambda x: " " .join(x))

combinedf['cleanFinal_joined'][5]

#veri gorsellestirme
grafic = combinedf.groupby('real').apply(lambda x: x['title'].count()).reset_index(name='counts')
grafic.real.replace({0:'false', 1:'true'}, inplace= True)
fig=px.bar(grafic, x='real', y='counts', color='real', barmode='group', height=400)
fig.show()

#konulara gore havee sayisi
numberOfNews = combinedf.groupby('subject').apply(lambda x: x['title'].count()).reset_index(name='counts')
fig = px.bar(numberOfNews, x='subject', y='counts', color='counts', title='Number Of News By Subject')
fig.show()

plt.figure(figsize=(24,22))

trueNewNumberOfWords = WordCloud(max_words=2100, width=1800, height=1000, stopwords=stopwords).generate(''.join(combinedf[combinedf.real==1].cleanFinal_joined))
plt.imshow(trueNewNumberOfWords, interpolation ='bilinear')

plt.figure(figsize=(24,22))

falseNewNumberOfWords = WordCloud(max_words=2100, width=1800, height=1000, stopwords=stopwords).generate(''.join(combinedf[combinedf.real==0].cleanFinal_joined))
plt.imshow(falseNewNumberOfWords, interpolation ='bilinear')

#Logistic regrssion model olusturma Model 1 metinsel ifadeyi sayisala ceitrmek icin CountVectorezer kullanilir
x_train, x_test, y_train, y_test = train_test_split(combinedf.cleanTitle_joined, combinedf.real, test_size=0.2, random_state=42)
vac_train = CountVectorizer().fit(x_train)
x_train_vectorized = vac_train.transform(x_train)
x_test_vectorized = vac_train.transform(x_test)
model = LogisticRegression(C=3)
model.fit(x_train_vectorized, y_train)
traditional = model.predict(x_test_vectorized)
accuracy = roc_auc_score(y_test, traditional)
print(accuracy)

#Model 2 metinsel ifadeyi sayisala cevirmek icin Keras dan tokenizer kullanilir
max_words=10000
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(x_train)

x_train=tokenizer.texts_to_sequences(x_train)
x_test=tokenizer.texts_to_sequences(x_test)

x_train=tf.keras.preprocessing.sequence.pad_sequences(x_train, padding='post', maxlen=256)
x_test=tf.keras.preprocessing.sequence.pad_sequences(x_test, padding='post', maxlen=256)

#relu, softmax , elu, adam, adamax en sik kullanilan fonksiyonlar
model= tf.keras.Sequential([
    tf.keras.layers.Embedding(max_words, 128),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0,5),
    tf.keras.layers.Dense(1)
])
model.summary()

